{"cells":[{"source":"---\ntitle: \"Pricing used cars to improve sales\"\nauthor: \"Gustavo Alejandro Garduño Macedo\"\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---","metadata":{},"id":"fa57be76","cell_type":"raw"},{"source":"install.packages(\"devtools\")\ndevtools::install_github(\"gadenbuie/rsthemes\")\nrsthemes::install_rsthemes()\nrstudioapi::applyTheme(\"Oceanic Plus {rsthemes}\")","metadata":{"eval":false,"tags":["remove_cell"]},"id":"71a297ea","cell_type":"code","execution_count":null,"outputs":[]},{"source":"# Packages \n\ninstall.packages(\"ggridges\")\ninstall.packages(\"corrr\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"xgboost\")\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(ggridges)\nlibrary(corrr)\nlibrary(tidymodels)\nlibrary(kableExtra)\nlibrary(patchwork)\n\n# Helpers\nknitr::opts_chunk$set(message = FALSE,\n                      warning = FALSE,\n                      echo = FALSE)\n\ntheme_set(theme_minimal())+\n  theme_update(axis.title = element_blank())","metadata":{"message":false,"tags":["remove_cell"],"warning":false},"id":"594addfa","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## Problem definition\n\nThe first step in this analysis is to understand and define the problem that we are trying to solve. It has been mentioned that at Discount Motors (a used car dealership in the UK ) the recently hired junior sales people have difficulties pricing used cars that arrive at the dealership.\n\nThis has caused sales to decline 18% in recent months,so in this analysis we are going to develop a machine learning model to price cars that arrive with more precision. It is known that cars that are more than £1500 above the estimated price will not sell. So the intention is to keep the model predictions within this range as much as possible.\n\n## Data\n\nWe are going to work with the Toyota specialist to test this solution approach. The specialists have collected data from other retailers on the price that a range of Toyota cars were listed at.\n\nA data description is presented here:\n\n| Feature      | Description                                    |\n|--------------|------------------------------------------------|\n| model        | Model name of the Toyota car                   |\n| transmission | Transmission type of the car                   |\n| fuelType     | The fuel type the car engine uses              |\n| year         | Year when the car was launched                 |\n| price        | Price of the car from retailers                |\n| mileage      | Number of miles that the car has been traveled |\n| tax          | Sales tax of the car                           |\n| mpg          | Miles traveled per gallon of fuel consumed     |\n| engineSize   | Size of the engine                             |\n\n: Table 1: Features description\n\nA summary and a first look at the data is also presented:","metadata":{},"id":"0ab00e95","cell_type":"markdown"},{"source":"toyota <- read_csv(\"data/toyota.csv\")\n\ntoyota %>% \n  skim_without_charts() %>% \n  kbl() %>% \n  kable_styling()\n\ntoyota %>% \n  slice_sample(n = 10) %>% \n  kbl() %>% \n  kable_styling()\n\n\n# Changing column names to snake case\n\ntoyota <- toyota %>% \n  clean_names()","metadata":{},"id":"cf3d3986","cell_type":"code","execution_count":null,"outputs":[]},{"source":"If we look at the summary, we notice that:\n\n- There are not missing data.\n- We have numerical and categorical variables.\n- There are not clear data inconsistencies at the moment.\n\nThe only cleaning step we perform is to change the column names into a snake case format (i.e. column_name ) to keep simplicity in performing the analysis.\n\nNow that we have defined the problem, looked the description of our data and made a  initial sanity check of our data its time to start with the Exploratory Data Analysis (EDA).\n\n## Exploratory data analysis\n\nIf we analyze the type of problem we are dealing with, we conclude:\n\n- The objective is to predict the sale price from different characteristics of the used car like model, the year of the release, the miles traveled, fuel type, transmission etc;\n\n- So we have a regression problem, our outcome is the price and the predictors are the features mentioned above, the prediction must be as precise as possible with the data we have.\n\nIt is known by theory that the next features are possibly related with the price of a car\n\n- Years: With more years since the release of the car, is possible that the price is lower.\n\n- Model: Sport models may have higher prices than other models\n\n- The relation with other variables is not clear, so we need to explore it.\n\nIn order to do so, we analyze some of the features of the data to accept or reject the prior information we have and determine what features are going to be included in the model. ","metadata":{"lines_to_next_cell":0},"id":"9112fb5c","cell_type":"markdown"},{"source":"\n# Helper function to plot numerical and categorical vars\n\nplot_num_cat <- function(tbl,num,cat,type){\n  if (type == \"box\") {\n    tbl %>% \n      ggplot(aes(x = fct_reorder({{cat}},{{num}}),\n                 y = {{num}}))+\n      geom_boxplot()+\n      coord_flip()\n  }\n  else if(type == \"violin\"){\n    tbl %>% \n      ggplot(aes(x = {{num}},\n                 y = fct_reorder({{cat}},{{num}})))+\n      geom_violin()\n  }\n  else if(type == \"ridges\"){\n    tbl %>% \n      ggplot(aes(x = {{num}},\n                 y = fct_reorder({{cat}},{{num}})))+\n      stat_density_ridges(quantile_lines = T)\n  }\n}\n","metadata":{"tags":["remove_cell"]},"id":"d798d76b","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### Distribution of price\n\nFirstly lets take a look at the distribution of the price:","metadata":{},"id":"1d7a39e6","cell_type":"markdown"},{"source":"# No transformation\np1 <- toyota %>% \n  ggplot(aes(x = price))+\n  geom_histogram(color = \"white\")+\n  ggtitle(\"Distribution of price\",\n          subtitle = \"No transformation\")\n\n# Log transformation on price \np2 <- toyota %>% \n  ggplot(aes(x = price))+\n  geom_histogram(color = \"White\")+\n  scale_x_continuous(trans = \"log10\")+\n  ggtitle(\"Distribution of price\",\n          subtitle = \"log10 transformation\")\n\n(p1+p2)+\n  plot_annotation(caption = \"Plot 1: Comparing distribution of price\")","metadata":{},"id":"e4a83767","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The distribution of price looks clearly not normal, so a log 10 transformation may be needed in order to get more accurate results, so now on we are going to work with the log of the price.","metadata":{},"id":"46b751f2","cell_type":"markdown"},{"source":"# Creating log 10 of price\n\ntoyota <- toyota %>% \n  mutate(price_log10 = log10(price))","metadata":{},"id":"a042e311","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### Comparing price across categories\n\nIn this section we are going to compare the price to different categorical variables to identify patterns that should be captured by our model.\n\n#### Price and the diferent car models","metadata":{},"id":"cc439b8c","cell_type":"markdown"},{"source":"toyota %>% \n  plot_num_cat(price,model, type = \"ridges\")+\n  scale_x_log10()+\n  theme(axis.title.x = element_text())+\n  labs(x = \"Price (log 10 scale)\")+\n  ggtitle(\"Prices by type of model\")","metadata":{},"id":"7c3b7de2","cell_type":"code","execution_count":null,"outputs":[]},{"source":"If we look at the plot, we can spot a difference in the price range distribution depending on the model of the car, the \"Supra\" a sport Toyota car has a higher median price than other more urban cars like \"Yaris\". So the model of the car should be considered as a feature of our model.\n\n### Price and transmission","metadata":{},"id":"f2c0b072","cell_type":"markdown"},{"source":"toyota %>% \n  tabyl(transmission) %>%\n  kbl() %>% \n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 2: Count of transmission type\")\n  \n  \n\ntoyota %>%\n  filter(transmission != \"Other\") %>% \n  plot_num_cat(price,transmission, type = \"box\")+\n  scale_y_log10()+\n  theme(axis.title.x = element_text())+\n  labs(y = \"Price\",\n       caption = \"Plot 2: Comparing the distribution of transmission type\")+\n  ggtitle(\"Price by transmission type\")","metadata":{},"id":"7bf866ef","cell_type":"code","execution_count":null,"outputs":[]},{"source":"If we look at Table 2 we see that the transmission type other and semi-auto are under represented in comparison to automatic and manual transmission groups, so we should consider make just one level for the under represented transmission types.\n\nNow, if we observe now at Plot 2 we can make the following insights:\n\n-   Higher prices are associated to automatic cars\n-   The difference between semi-auto and manual cars is not that clear.\n\n### Price and fuel type","metadata":{},"id":"3f02389a","cell_type":"markdown"},{"source":"toyota %>% \n  tabyl(fuel_type) %>% \n  kbl() %>% \n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 3: Count of fuel type\")\n\ntoyota %>%\n  plot_num_cat(price,fuel_type,type = \"box\")+\n  scale_y_log10()+\n  theme(axis.title.x = element_text())+\n  labs(y = \"Price\",\n       caption = \"Plot 3: Price by fuel type\")+\n  ggtitle(\"Comparing price for different fuel types\")","metadata":{},"id":"0da42b84","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Looking at Plot 3 clearly there is a pattern, hybrid type cars have a higher median price than diesel and petrol,then the type of fuel should be considered in our model.\n\nNotice that the groups of diesel and other are under represented in the data set in comparison of the hybrid and diesel groups, so like the model variable we could consider group other and diesel together.\n\n### Prices across the diferent years","metadata":{},"id":"ad75682b","cell_type":"markdown"},{"source":"toyota %>% \n  group_by(year) %>% \n  summarise(median_price = median(price),\n            sd = sd(price)) %>% \n  filter(year>2002) %>% \n  ggplot(aes(x = year, y = median_price))+\n  geom_line()+\n  geom_point()+\n  geom_errorbar(aes(ymin=median_price-sd, ymax=median_price+sd), width=.2,\n                 position=position_dodge(0.05))+\n  scale_y_log10()+\n  theme(axis.title.y = element_text())+\n  labs(y = \"Price (log 10 scale)\",\n       caption = \"Plot 3. Median prices by year (with error bars showing the dispersion of the distribution)\")+\n  ggtitle(\"Comparing median prices by year\")\n\n","metadata":{},"id":"8f33253b","cell_type":"code","execution_count":null,"outputs":[]},{"source":"As expected we spot a positive trend, the median prices have been increasing as the years pass so the year of the car is an important feature in our model.\n\n## Comparing prices across numerical variables\n\nThe next step in our analysis is to determine what numerical variables could be useful predictor for the price of the car.\n\n### Correlations across numerical variables\n\nLets look at the correlation of the numerical variables:","metadata":{},"id":"eef3d3a0","cell_type":"markdown"},{"source":"cor <- toyota %>% \n  select_if(is.numeric) %>% \n  select(-price) %>% \n  correlate() %>% \n  arrange()\n\ncor %>% \n  kbl() %>% \n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 4: Correlations for numerical vars\")\n  \ncor %>% \n  network_plot(repel = T)","metadata":{},"id":"25ccfbce","cell_type":"code","execution_count":null,"outputs":[]},{"source":"If we take a look at the correlation of the variables we conclude that:\n\n- The engine size could provide some information about the price, so its worth to analyze their scatter plot\n- The tax and miles per gallon provide almost no information about the price, so they wont be considered in the model\n- Price and mileage provide useful information about the price, again we will analyze their scatter plot.\n\n### Prices and mileage","metadata":{},"id":"03f9ef6f","cell_type":"markdown"},{"source":"toyota %>% \n  ggplot(aes(x = mileage, y = price))+\n  geom_point(alpha = 0.3, size = 0.5)+\n  scale_y_log10()+\n  stat_smooth(method = \"lm\")+\n  theme(axis.title = element_text())+\n  labs(y = \"Price (log 10 scale)\",\n       caption = \"Plot 5: Comparing price and mileage\")+\n  ggtitle(\"Price vs Mileage\")","metadata":{},"id":"976e279a","cell_type":"code","execution_count":null,"outputs":[]},{"source":"As expected there is a clear negative relationship in the number of miles traveled with the car price\n\n-   On average, the higher the mileage the lower the price.\n\nBut notice from the correlation table that the two variables (year and mileage) are capturing the same information. So it would be a good practice to drop one of this correlated variables when making the model.\n\n### Prices and engine size.","metadata":{},"id":"576e810b","cell_type":"markdown"},{"source":"toyota %>% \n  filter(engine_size > 0, engine_size < 4.2) %>% \n  mutate(engine_size = as.character(engine_size)) %>% \n  ggplot(aes(x = fct_reorder(engine_size,price),\n             y = price))+\n  geom_boxplot()+\n  scale_y_log10()+\n  theme(axis.title = element_text())+\n  labs(y = \"Price (log 10 scale)\",\n       x = \"Engine size\",\n       caption = \"Plot 6: Comparing prices and engine size\")+\n  ggtitle(\"Prices by engine size\")","metadata":{},"id":"bde2f3ad","cell_type":"code","execution_count":null,"outputs":[]},{"source":"There is a clear pattern higher engine_size, then higher the price of the car on average (engine size of zero and less than 4.2 were omitted because they have only one observation). The engine size variable should be considered in out model.\n\n## Data modeling\n\nWith the observed in the exploratory data analysis (EDA) we conclude that the most important variables that affect in the log price are:\n\n-   Year\n-   Mileage\n-   Engine size\n-   Model\n-   Transmission\n-   Fuel type\n\nBut, we notice earlier that year and mileage essentially capture the same information so we will not consider the mileage. And we wont consider interactions between predictors for simplicity.\n\nAs mentioned earlier given that we want to predict the log of the price when a new car arrives based on numerical and categorical variables we have a regression problem. And we are going to compare tree types of models that are suitable for regression tasks:\n\n- Linear Regression: A traditional, simpler model.\n- Regularized lasso regression: Since we have a lot of categorical predictors and it may be a best option to let the model \"choose\" the relevant predictors.\n- Boosted Trees (XGBoost): To make a more robust model, and possibly improve prediction capabilities.\n\nRemember that when we start our analysis we said that cars that are more than £1500 above the estimated price will not sell. So the intention is to keep the model predictions within this range as much as possible, we are going to do this by choosing the rmse (Root Mean Squared Error) as our metric to compare the tree models. The lower the rmse, better precision of our predictions.\n\n### Spending data budget\n\nWe split our data stratifying by Price (wich means that we want to keep almost the same distribution of prices in the train and test sets) keeping 75% of our data to estimate the parameters of our model and 25% to test the model on data that has never seen to see how well it would perform on new data.\n\nLets take a look at our training and testing data sets","metadata":{},"id":"d225d72a","cell_type":"markdown"},{"source":"# Setting a seed for reproducibility\nset.seed(123)\n\nspl <- toyota %>%\n  initial_split(strata = price_log10)\n\n# Train set\ntrain <- spl %>% \n  training()\n\ntrain %>% \n  slice_sample(n = 6) %>% \n  kbl() %>% \n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 5: Training set\")\n\n  \n# Test set\ntest <- spl %>% \n  testing()\n\ntest %>% \n  slice_sample(n = 6) %>% \n  kbl() %>% \n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 6: Test set\")","metadata":{},"id":"9b765736","cell_type":"code","execution_count":null,"outputs":[]},{"source":"We are going to estimate the model parameters with ~ 5000 observations and testing it on ~ 1600 observations.\n\nTo validate the model we are going to use a 10-fold cross validation.","metadata":{},"id":"7f4722ae","cell_type":"markdown"},{"source":"set.seed(456)\n\nfolds <- vfold_cv(train, strata = price_log10,v = 10)","metadata":{},"id":"f213725a","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### Model selection\n\nBefore actually train and validate our models we need to do the next pre-procesing steps and feature engenieering, according to what we observed in the EDA:\n\n- Drop unusual values for the engine size variable (engines with size zero and groups of engine size of only one observation).\n- Group unrepresented categories for transmission, fuel type and also by model to reduce the number of predictors.\n- Make dummy variables.","metadata":{},"id":"263c8688","cell_type":"markdown"},{"source":"# Pre-procesing steps, feature engineering\nbase_rec <- recipe(price_log10 ~ \n                        year + engine_size + model+\n                        transmission + fuel_type,\n                      data = train) %>% \n  step_other(transmission,fuel_type,model,\n             threshold = .05) %>% \n  step_filter(engine_size > 0, engine_size < 4.2) %>% \n  step_dummy(all_nominal_predictors())","metadata":{},"id":"279c2c2f","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### Model specification\n\n- For linear regression: A usual linear regression model\n- For lasso regression: Here we want to determine the best value for the penalty, that indicate what features are going to have more impact on the model. \n- For boosted trees: Here we want to determine the number of trees to have the lower value of rmse possible.\n","metadata":{},"id":"3417787a","cell_type":"markdown"},{"source":"# Model specifications\n\nlin_reg_spec <- linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\n\nlasso_reg_spec <- linear_reg(penalty = tune(),\n                             mixture = 1) %>%\n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\")\n\nxgb_spec <- boost_tree(trees = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"xgboost\")","metadata":{},"id":"19910beb","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### Model validation\n\nWe are going to determine the parameters for the penalty (lasso regression) and the number of trees (boosted trees) using a grid search among the possible values in the parameter space.","metadata":{},"id":"a34672d1","cell_type":"markdown"},{"source":"# workflows\nlin_reg_wf <- workflow(base_rec,lin_reg_spec)\nlasso_reg_wf <- workflow(base_rec, lasso_reg_spec)\nxgb_wf <- workflow(base_rec,xgb_spec)\n\n# grids for parameter search\nlasso_reg_grid <- grid_regular(penalty(\n  range = c(-5,0)),\n  levels = 10)\n\nxgb_grid <- grid_regular(\n  trees(c(500,2000)),\n  levels = 10\n)\n\n# metric set\nm_set <- metric_set(rmse)","metadata":{},"id":"4eeeb047","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Now we train the model in 10 different subsets of our training data and validating in 10 other different subsets of our training data to see how would the model perform in the test data (data that has never seen before).\n\nWe observe the next results:","metadata":{},"id":"055298e3","cell_type":"markdown"},{"source":"# Model validation\n\n# Linear regression\nlin_reg_res <- lin_reg_wf %>% \n  fit_resamples(resamples = folds,\n                metrics = m_set)\n\nlin_reg_metrics <-lin_reg_res %>% \n  collect_metrics() %>% \n  select(.metric,mean,std_err) %>% \n  mutate(model = \"linear_regression\")\n\n# Lasso regression\nset.seed(789)\n\nlasso_reg_res <- tune_grid(\n  lasso_reg_wf,\n  resamples = folds,\n  metrics = m_set,\n  grid = lasso_reg_grid\n)\n\n\n# Selecting a simpler model with the best rmse\n\nlasso_reg_metrics <-\n  lasso_reg_res %>% \n  select_by_one_std_err(metric = \"rmse\", desc(penalty)) %>%\n  select(.metric,mean,std_err) %>% \n  mutate(model = \"lasso_regression\")\n\n# XGBoost\n\nxgb_res <- tune_grid(\n  xgb_wf,\n  resamples = folds,\n  metrics = m_set,\n  grid = xgb_grid\n)\n\nxgb_metrics <- xgb_res %>% \n  show_best(n = 1) %>% \n  select(.metric,mean,std_err) %>% \n  mutate(model = \"XGBoost\")\n\nmodel_comp <- lin_reg_metrics %>% \n  bind_rows(lasso_reg_metrics) %>% \n  bind_rows(xgb_metrics)","metadata":{"message":false,"tags":["remove_output"],"warning":false},"id":"ddd47792","cell_type":"code","execution_count":null,"outputs":[]},{"source":"model_comp %>% \n  kbl() %>% \n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 6: Model results comparision\")\n  \nmodel_comp %>%\n  ggplot(aes(y = mean, x = model, color = model))+\n  geom_point()+\n  geom_errorbar(aes(ymin=mean-std_err,\n                    ymax=mean+std_err),\n                width=.2,\n                position=position_dodge(0.05))+\n  theme(legend.position = \"none\",\n        axis.title.y = element_text())+\n  labs(y = \"rmse\",\n       caption = \"Plot 7: Comparing models (with error bar for the estimators\")+\n  ggtitle(\"Comparing model validation performance\")","metadata":{},"id":"a17ae98a","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Despite the linear regression preformed even better than lasso regression, since we want prediction power we will keep with the XGBoost model.\n\n### Model selection\n\nNow lets see of the XGBoost can be further improved increasing the number of trees search (i.e. making a bigger grid for the parameter).","metadata":{},"id":"7ddb2386","cell_type":"markdown"},{"source":"# Tuning the numer of trees\n\nxgb_tune_spec <- boost_tree(trees = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"xgboost\")\n\n# workflow\nxgb_tune_wf <- workflow(base_rec,xgb_tune_spec)\n\n# bigger grid\nxgb_tune_grid <- grid_regular(trees(),\n                              levels = 15)\n\n# validation (it may take some minutes)\nset.seed(1011)\n\nxgb_tune_res <- tune_grid(\n  xgb_tune_wf,\n  resamples = folds,\n  metrics = m_set,\n  grid = xgb_tune_grid\n)\n","metadata":{"message":false,"tags":["remove_output"],"warning":false},"id":"cb20025f","cell_type":"code","execution_count":null,"outputs":[]},{"source":"xgb_tune_res %>% \n  show_best() %>% \n  select(trees,.metric,mean,std_err) %>% \n  kbl() %>% \n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 7: XGBoost comparision\")","metadata":{},"id":"4112fb2a","cell_type":"code","execution_count":null,"outputs":[]},{"source":"We see that there are no further improvements in the rmse metric above the ~140 trees so we should keep the simpler model.\n\n> So the final model selection is an XGBoost with 143 trees.\n\n### Modeling results\n\nWe now fit the selected model to all the training test and see how well it performed on data that has never seen before.","metadata":{},"id":"47b95796","cell_type":"markdown"},{"source":"final_xgb <- xgb_tune_res %>% \n  select_best()\n\nfinal_res <- xgb_tune_wf %>% \n  finalize_workflow(final_xgb) %>% \n  last_fit(spl)\n\ncollect_metrics(final_res) %>% \n  kbl() %>%\n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 8: XGBoost on new data\")","metadata":{},"id":"6fda2121","cell_type":"code","execution_count":null,"outputs":[]},{"source":"We now see that our XGBoost model performed in the test set (0.5718 RMSE) almost as good as in the training set (0.5211 RMSE).\n\nNow the final step is to check that this models predictions on new data beat the benchmark we set before: the predictions should not be in a range of £1500 above the estimated price or the car will not sell.\n\nBut remember that we predicted values for the log price, so we need to take back the predicted values to the original space using the inverse transformation.","metadata":{},"id":"272dcf84","cell_type":"markdown"},{"source":"benchmark <-\n  collect_predictions(final_res) %>% \n  select(.pred) %>% \n  rename(pred_log10 = .pred) %>% \n  bind_cols(\n    test %>% \n      select(price,price_log10)\n  ) %>% \n  mutate(pred = 10^pred_log10,\n         diff = price-pred,\n         is_in_range = ifelse(diff>=-1500,1,0))\n\nbenchmark %>% \n  ggplot(aes(x = pred, y = price))+\n  geom_point(alpha = 0.8, size = 0.8)+\n  geom_abline(slope = 1, intercept = 0, color = \"steelblue\")+\n  theme(axis.title = element_text())+\n  labs(caption = \"Plot 8: Comparing model actual prices vs predicted on the test set\")+\n  ggtitle(\"Actual prices vs Predicted\")\n\nbenchmark %>% \n  tabyl(is_in_range) %>%\n  kbl() %>%\n  kable_minimal(full_width = F) %>%\n  footnote(general = \"Table 8: Are the predictions in the desired range ? \")","metadata":{},"id":"71137121","cell_type":"code","execution_count":null,"outputs":[]},{"source":"If we look at Plot 8 we have a better sense on how the model performed on new data, the model performed very good on new data. But note that for prices above ~  £40,000 the model has some troubles to predict new data.\n\nNow the Table 8 indicate us that 93% percent of our predictions on new data are in a range of £1500 below the estimated price. So we can conclude that 93% of Toyota cars that arrive at the new dealerships will be sold.","metadata":{},"id":"3755ff6f","cell_type":"markdown"},{"source":"# Saving the model for future predictions or work\n\nfinal_wf <- extract_workflow(final_res)\n\nsaveRDS(final_wf, \"final_wf.rds\")","metadata":{},"id":"3fc9f4e2","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## Conclusions\n\nRecapitulating, we developed a XGBoost model for predicting prices for Toyota cars that arrive to the dealerships. This model will help the Jr. sales people to price the new cars more effectively, which means that more cars will be sold.\n\nOn this test the model correctly predicted 93% of Toyota cars that has never seen before in the desired range, making this a good indicator to use the model for other car brands, and implemented on the diferent dealerships.\n\n## Further actions\n\n- Despite this is a good model if our desire is to implement it on more brands, new features will be needed and a model adjust may be necessary.\n- The model performance needs to be tracked over the time, and it may need adjustments due to changes in the market.\n- It may worth the while to consider model interactions between predictors (which was not contemplated in this analysis)\n- The stakeholder with its experience should consider tell us to incorporate more features or provide more market knowledge in order to improve the predictions and then make better data-driven decisions.\n","metadata":{},"id":"eb83193b","cell_type":"markdown"}],"metadata":{"jupytext":{"cell_metadata_filter":"eval,message,tags,warning,-all","main_language":"R","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}