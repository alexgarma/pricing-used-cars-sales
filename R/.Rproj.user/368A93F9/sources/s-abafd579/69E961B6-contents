---
title: "Pricing used cars to improve sales"
author: "Gustavo Alejandro Garduño Macedo"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r eval=FALSE, include=FALSE}
# Modify RStudio apperance

# install.packages("devtools")
# devtools::install_github("gadenbuie/rsthemes")
# rsthemes::install_rsthemes()
# rstudioapi::applyTheme("Oceanic Plus {rsthemes}")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Helpers 

install.packages("ggridges")
install.packages("corrr")
install.packages("glmnet")
install.packages("xgboost")

library(tidyverse)
library(skimr)
library(janitor)
library(ggridges)
library(corrr)
library(tidymodels)
library(kableExtra)
library()

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

theme_set(theme_minimal())+
  theme_update(axis.title = element_blank())
```

## Problem definition

The first step in this analysis is to understand and define the problem that we are trying to solve. It has been mentioned that at Discount Motors (a used car dealership in the UK ) the recently hired junior sales people have difficulties pricing used cars that arrive at the dealership.

This has caused sales to decline 18% in recent months,so in this analysis we are going to develop a machine learning model to price cars that arrive with more precision. It is known that cars that are more than £1500 above the estimated price will not sell. So the intention is to keep the model predictions within this range as much as possible.

## Data

We are going to work with the Toyota specialist to test this solution approach. The specialists have collected data from other retailers on the price that a range of Toyota cars were listed at.

A data description is presented here:

| Feature      | Description                                    |
|--------------|------------------------------------------------|
| model        | Model name of the Toyota car                   |
| transmission | Transmission type of the car                   |
| fuelType     | The fuel type the car engine uses              |
| year         | Year when the car was launched                 |
| price        | Price of the car from retailers                |
| mileage      | Number of miles that the car has been traveled |
| tax          | Sales tax of the car                           |
| mpg          | Miles traveled per gallon of fuel consumed     |
| engineSize   | Size of the engine                             |

: Table 1: Features description

A summary and a first look at the data is also presented:

```{r}
toyota <- read_csv("data/toyota.csv")

toyota %>% 
  skim_without_charts() %>% 
  kbl() %>% 
  kable_styling()

toyota %>% 
  slice_sample(n = 10) %>% 
  kbl() %>% 
  kable_styling()


# Changing column names to snake case

toyota <- toyota %>% 
  clean_names()
```

If we look at the summary, we notice that:

- There are not missing data.
- We have numerical and categorical variables.
- There are not clear data inconsistencies at the moment.

The only cleaning step we perform is to change the column names into a snake case format (i.e. column_name ) to keep simplicity in performing the analysis.

Now that we have defined the problem, looked the description of our data and made a  initial sanity check of our data its time to start with the Exploratory Data Analysis (EDA).

## Exploratory data analysis

If we analyze the type of problem we are dealing with, we conclude:

- The objective is to predict the sale price from different characteristics of the used car like model, the year of the release, the miles traveled, fuel type, transmission etc;

- So we have a regression problem, our outcome is the price and the predictors are the features mentioned above, the prediction must be as precise as possible with the data we have.

It is known by theory that the next features are possibly related with the price of a car

- Years: With more years since the release of the car, is possible that the price is lower.

- Model: Sport models may have higher prices than other models

- The relation with other variables is not clear, so we need to explore it.

In order to do so, we analyze some of the features of the data to accept or reject the prior information we have and determine what features are going to be included in the model. 

```{r include=FALSE}

# Helper function to plot numerical and categorical

plot_num_cat <- function(tbl,num,cat,type){
  if (type == "box") {
    tbl %>% 
      ggplot(aes(x = fct_reorder({{cat}},{{num}}),
                 y = {{num}}))+
      geom_boxplot()+
      coord_flip()
  }
  else if(type == "violin"){
    tbl %>% 
      ggplot(aes(x = {{num}},
                 y = fct_reorder({{cat}},{{num}})))+
      geom_violin()
  }
  else if(type == "ridges"){
    tbl %>% 
      ggplot(aes(x = {{num}},
                 y = fct_reorder({{cat}},{{num}})))+
      stat_density_ridges(quantile_lines = T)
  }
}

```

### Distribution of price

Firstly lets take a look at the distribution of the price:

```{r}
# No transformation

p1 <- toyota %>% 
  ggplot(aes(x = price))+
  geom_histogram(color = "white")+
  ggtitle("Distribution of price",
          subtitle = "No transformation")

# Log transformation on price 
p2 <- toyota %>% 
  ggplot(aes(x = price))+
  geom_histogram(color = "White")+
  scale_x_continuous(trans = "log10")+
  ggtitle("Distribution of price",
          subtitle = "log10 transformation")


```

The distribution of price is not normal, so a log 10 transformation may be needed in order to get more accurate results, so now on we are going to work with the log of the price.

```{r}
toyota <- toyota %>% 
  mutate(price_log10 = log10(price))
```

### Comparing price across categories

#### Price and the diferent car models

```{r}
toyota %>% 
  tabyl(model)

toyota %>% 
  plot_num_cat(price,model, type = "ridges")+
  scale_x_log10()+
  theme(axis.title.x = element_text())+
  labs(x = "Price")+
  ggtitle("Prices by type of model")
```

We see a clearly difference in the price range across the model of the car, so we need to consider the model of the car in our modeling process

### Price and transmission

```{r}
toyota %>% 
  tabyl(transmission)

toyota %>%
  filter(transmission != "Other") %>% 
  plot_num_cat(price,transmission, type = "box")+
  scale_y_log10()+
  ggtitle("Price by transmission type")
```

Note that there is only one car with transmission type other so we could drop that group.

-   Higher prices are associated to automatic cars
-   The difference between semi-auto and manual cars are not that clear.

We could create a new binary variable, to indicate just if a car is automatic or not since there is no appreciable difference between semi-auto and manual cars.

### Price and fueltype

```{r}
toyota %>% 
  tabyl(fuel_type)

toyota %>%
  plot_num_cat(price,fuel_type,type = "box")
```

Clearly there is a pattern, hybrid type cars have a higher median price than diesel and petrol, but if we look the groups of diesel and other we notice that are under represented in comparison of the hybrid and diesel groups.

So it might be a better approach to consider only Hybryd and Diesel cars and drop the remaining levels to the Other category.

## Prices across the diferent years

```{r}
toyota %>% 
  ggplot(aes(x = year, y = price, group = year))+
  geom_boxplot()+
  scale_y_log10()+
  ggtitle("Prices by year")

# Note: lineplot with error bar
```

As expected the median prices have been increasing as the years pass so the year of the car is an important feature in our model.

## Comparing prices across numerical variables

### Correlations across numerical variables

```{r}
toyota %>% 
  select_if(is.numeric) %>% 
  select(-price) %>% 
  correlate() %>% 
  arrange() %>% 
  network_plot()
```

### Prices and mileage

```{r}
toyota %>% 
  ggplot(aes(x = mileage, y = price))+
  geom_point()+
  scale_y_log10()+
  stat_smooth(method = "lm")+
  theme(axis.title = element_text())
```

As expected there is a clear negative relationship in the number of miles traveled with the car price

-   On average, the higher the mileage the lower the price.

But notice that if we compare this with the prices across years the two variables are capturing the same information. So it would be a good practice to drop one of this correlated variables

```{r}
toyota %>% 
  ggplot(aes(x = mileage, y = year))+
  geom_jitter()+
  scale_x_log10()+
  theme(axis.title = element_text())+
  ggtitle("Mileage vs Year")
```

But there is a no linear relationship between mileage and year (BUT IF WE DROP ONE WE MAY NOT NEED THIS)

### Prices and engine size.

```{r}
toyota %>% 
  tabyl(engine_size)

toyota %>% 
  filter(engine_size > 0, engine_size < 4.2) %>% 
  mutate(engine_size = as.character(engine_size)) %>% 
  ggplot(aes(x = fct_reorder(engine_size,price),
             y = price))+
  geom_boxplot()+
  scale_y_log10()+
  theme(axis.title = element_text())
```

There is a clear pattern higher engine_size, then higher the price of the car on average (engine size of zero and less than 4.2 were omitted because they have only one observation).

## Data modeling

With the observed in the EDA we conlude that the most important variable that affect in the log price are:

-   Year
-   Mileage
-   Engine size
-   Model
-   Transmission
-   Fuel type

But, we notice that year and mileage essentially capture the same information so we will not consider the mileage. And we wont consider interactions between predictors.

Given that we want to predict the log of the price when a new car arrives based on numerical and categorical variables we have a regression problem. And we are going to compare thee types of models:

-   Linear Regression
-   Regularized regression: Since we have a lot of categorical predictors and it may be a best option to let the model "choose" the relevant predictors.

### Spending data budget

we split our data stratifying by Price (wich means that we want to keep almost the same distribution of prices in the train and test sets) keeping 75% of our data to estimate the parameters of our model and 25% to evaluate the model.

```{r}
set.seed(123)

spl <- toyota %>%
  initial_split(strata = price_log10)

train <- spl %>% 
  training()

test <- spl %>% 
  testing()

```

To validate the model we are going to use a 10-fold cross validation.

```{r}
set.seed(456)

folds <- vfold_cv(train, strata = price_log10,v = 10)
```

### Model selection

Pre-procesing steps and feature engenieering

```{r}
base_rec <- recipe(price_log10 ~ 
                        year + engine_size + model+
                        transmission + fuel_type,
                      data = train) %>% 
  step_other(transmission,fuel_type,model,
             threshold = .05) %>% 
  step_filter(engine_size > 0, engine_size < 4.2) %>% 
  step_dummy(all_nominal_predictors())
```

Model specification

```{r}
lin_reg_spec <- linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

lasso_reg_spec <- linear_reg(penalty = tune(),
                             mixture = 1) %>%
  set_mode("regression") %>% 
  set_engine("glmnet")

xgb_spec <- boost_tree(trees = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost")
```

Model validation

```{r}
# Workflows
lin_reg_wf <- workflow(base_rec,lin_reg_spec)
lasso_reg_wf <- workflow(base_rec, lasso_reg_spec)
xgb_wf <- workflow(base_rec,xgb_spec)

# Grids
lasso_reg_grid <- grid_regular(penalty(
  range = c(-5,0)),
  levels = 10)

xgb_grid <- grid_regular(
  trees(c(500,2000)),
  levels = 10
)


# Metric set
m_set <- metric_set(rmse)
```

Linear regression

```{r}
lin_reg_res <- lin_reg_wf %>% 
  fit_resamples(resamples = folds,
                metrics = m_set)

lin_reg_res %>% 
  collect_metrics()
```

Lasso Regression

```{r}
set.seed(789)

lasso_reg_res <- tune_grid(
  lasso_reg_wf,
  resamples = folds,
  metrics = m_set,
  grid = lasso_reg_grid
)

lasso_reg_res %>% 
  show_best()

lasso_reg_res %>% 
  select_by_one_std_err(metric = "rmse", desc(penalty))
```

XGBoost

```{r}
xgb_res <- tune_grid(
  xgb_wf,
  resamples = folds,
  metrics = m_set,
  grid = xgb_grid
)

xgb_res %>% 
  show_best()
```

We keep with XGBoost

```{r}
xgb_tune_spec <- boost_tree(trees = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost")

xgb_tune_wf <- workflow(base_rec,xgb_tune_spec)

xgb_tune_grid <- grid_regular(trees(),
                              levels = 15)

set.seed(1011)

xgb_tune_res <- tune_grid(
  xgb_tune_wf,
  resamples = folds,
  metrics = m_set,
  grid = xgb_tune_grid
)

xgb_tune_res %>% 
  show_best()
```

No improvements above 145 trees

Finalizing the workflow

```{r}
final_xgb <- xgb_tune_res %>% 
  select_best()

final_res <- xgb_tune_wf %>% 
  finalize_workflow(final_xgb) %>% 
  last_fit(spl)

collect_metrics(final_res)

collect_predictions(final_res) %>% 
  select(.pred) %>% 
  rename(pred_log10 = .pred) %>% 
  bind_cols(
    test %>% 
      select(price,price_log10)
  ) %>% 
  mutate(pred = 10^pred_log10,
         diff = price-pred,
         is_in_range = ifelse(diff>=-1500,1,0)) %>% 
  # tabyl(is_in_range)
  ggplot(aes(x = pred, y = price))+
  geom_point()+
  geom_abline(slope = 1, intercept = 0, color = "steelblue",
              size = 1)
```

```{r}
final_wf <- extract_workflow(final_res)

saveRDS(final_wf, "final_wf.rds")
```

## Conclusions

## Further actions
